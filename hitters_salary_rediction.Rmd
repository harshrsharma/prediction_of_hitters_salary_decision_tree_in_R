---
title: "Hitters Salary Prediction"
author: "Harsh Sharma"
date: "11/13/2019"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
```


```{r loadpackages, warning=FALSE, message=FALSE}
pacman::p_load(ISLR, MASS, rpart, rpart.plot, caret,ggplot2,randomForest, gbm, tree,leaps, mlbench, data.table)
theme_set(theme_classic())
```


#Question1 Remove the observations with unknown salary information. How many
#observations were removed in this process?
```{r}
data("Hitters")

hitters.df=setDF(Hitters)
hitters_data=na.omit(hitters.df,cols="Salary")
#no of observations removed = 322-263 = 59
```


#Question 2 Generate log-transform the salaries. Can you justify this transformation? 
```{r}
logSalary <- log(hitters_data$Salary)
hist(hitters_data$Salary)
hist(log(hitters_data$Salary,2))
```
#The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.
#The pattern above is more visible after log transformation


#Question3: Create a scatterplot with Hits on the y-axis and Years on the x-axis using all the
#observations. Color code the observations using the log Salary variable. What
#patterns do you notice on this chart, if any? 
```{r}
mid <- mean(logSalary)

ggplot(hitters_data, aes(x=hitters_data$Years,y=hitters_data$Hits,color=logSalary)) +  geom_point() + 
     xlab("Years") + 
      ylab(" Hits") + 
        ggtitle("years vs hits ?")

```

#Interesting pattern found from the graph is that players with high hits in their early years have #the highest of salary.

#Question4: Run a linear regression model of Log Salary on all the predictors using the entire
#dataset. Use regsubsets() function to perform best subset selection from the
#regression model. Identify the best model using BIC. Which predictor variables
#are included in this (best) model? 
```{r Lienar regression}
hitters_data$Salary <- NULL
hitters_data$logSalary <- logSalary
search <- regsubsets(hitters_data$logSalary~ ., data = hitters_data, nbest = 1, nvmax = dim(hitters_data)[2],
                     method = "exhaustive")
sum <- summary(search)

# show models
sum$which

# show metrics
sum$rsq
sum$adjr2
sum$cp
sum$bic
```
# Predictors included in best model are Hits, Years and Walks

#Question5 Now create a training data set consisting of 80 percent of the observations, and a
#test data set consisting of the remaining observations. 
```{r Splitting}

library("data.table")


# **Split the data into training (80%) and validation/test set (20%)**
set.seed(42)
training.index <- createDataPartition(hitters_data$logSalary, p = 0.8, list = FALSE)
Hitters.train <- hitters_data[training.index, ]
Hitters.valid <- hitters_data[-training.index, ]

```


#Question6 Generate a regression tree of log Salary using only Years and Hits variables from
# training data set. Which players are likely to receive highest salaries
#according to this model? Write down the rule and elaborate on it.
```{r RegressionTree}
# Generate regression tree
set.seed(42)
hitters.train.regtree <- rpart(Hitters.train$logSalary ~ Years + Hits, data = Hitters.train)


prp(hitters.train.regtree, type = 2,extra=1, under = TRUE, split.font = 2, 
    varlen = -10, box.palette = "BuOr")

rpart.rules(hitters.train.regtree, cover = TRUE ) # find rules

```
#ANSWER 6
# The players who have played atleast for 5 years and having hits greater than or equal to 118 are getting the highest salaries. 
# The rule is when	Years	>= 5	&	Hits >=	118. 31% of the players receive highest salaries.

#Question7 Now create a regression tree using all the variables in the training data set.
#Perform boosting on the training set with 1,000 trees for a range of values of the
#shrinkage parameter lambda. Produce a plot with different shrinkage values on the xaxis and the #corresponding training set MSE on the y-axis. 
```{r}
#Creating the tree with all the parameters

shrink_value = seq(.001, 0.01 , by = .0001)
train_set = rep(NA, length(shrink_value))
for (i in 1:length(shrink_value)){
    boost_tree = gbm(logSalary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = shrink_value[i])
    pred.train = predict(boost_tree, Hitters.train, n.trees = 1000)
    train_set[i] = mean((pred.train - Hitters.train$logSalary)^2) }
plot(shrink_value, train_set, type = "b", xlab = "Shrinkage values", ylab = "Training Set MSE")

```

#Question8 Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis
```{r}
##Plot for validation data set 
set.seed(42)
test_set <- rep(NA, length(shrink_value))
for (i in 1:length(shrink_value)) {
    boost_tree = gbm(logSalary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = shrink_value[i])
    yhat = predict(boost_tree, Hitters.valid , n.trees = 1000)
    test_set[i] = mean((yhat - Hitters.valid$logSalary)^2)
}
plot(shrink_value,test_set , type = "b", xlab = "Shrinkage values", ylab = "Test Set MSE")

```


#Q9. Which variables appear to be the most important predictors in the boosted
model?
```{r}
summary(boost_tree)
```

#In the above plot, we could infer that CAtBat is the most vital and imporatnt predictor in the model.


#Question 10 - Now apply bagging to the training set. What is the test set MSE for this approach?

```{r}
set.seed(42)
bagging <- randomForest(logSalary~., data = Hitters.train, importance = TRUE)
bagging_pred <- predict(bagging, Hitters.valid)
plot(bagging_pred, Hitters.valid$logSalary)
abline(0,1)
mean((bagging_pred - Hitters.valid$logSalary)^2)

```
